{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvEWn4vKICZOCouwbDOIfw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koushik980/NLP/blob/main/NLP_F_12_9_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sU26meUHM0V_",
        "outputId": "f0888878-3d6f-41df-a15e-02af13b4bc30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id keyword        location  \\\n",
            "0   0  ablaze             NaN   \n",
            "1   1  ablaze             NaN   \n",
            "2   2  ablaze   New York City   \n",
            "3   3  ablaze  Morgantown, WV   \n",
            "4   4  ablaze             NaN   \n",
            "\n",
            "                                                text  target  \n",
            "0  Communal violence in Bhainsa, Telangana. \"Ston...       1  \n",
            "1  Telangana: Section 144 has been imposed in Bha...       1  \n",
            "2  Arsonist sets cars ablaze at dealership https:...       1  \n",
            "3  Arsonist sets cars ablaze at dealership https:...       1  \n",
            "4  \"Lord Jesus, your love brings freedom and pard...       0  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Logistic Regression (TF-IDF) Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8804    0.9920    0.9329      1878\n",
            "           1     0.9051    0.3611    0.5162       396\n",
            "\n",
            "    accuracy                         0.8821      2274\n",
            "   macro avg     0.8927    0.6766    0.7246      2274\n",
            "weighted avg     0.8847    0.8821    0.8603      2274\n",
            "\n",
            "\n",
            "SVM (TF-IDF) Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8933    0.9941    0.9410      1878\n",
            "           1     0.9402    0.4369    0.5966       396\n",
            "\n",
            "    accuracy                         0.8971      2274\n",
            "   macro avg     0.9168    0.7155    0.7688      2274\n",
            "weighted avg     0.9015    0.8971    0.8810      2274\n",
            "\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.8165 - loss: 0.4666 - val_accuracy: 0.8830 - val_loss: 0.3387\n",
            "Epoch 2/5\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.9250 - loss: 0.2300 - val_accuracy: 0.8802 - val_loss: 0.3443\n",
            "Epoch 3/5\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9824 - loss: 0.0586 - val_accuracy: 0.8797 - val_loss: 0.4153\n",
            "Epoch 4/5\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.9958 - loss: 0.0209 - val_accuracy: 0.8830 - val_loss: 0.4694\n",
            "Epoch 5/5\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.9961 - loss: 0.0156 - val_accuracy: 0.8725 - val_loss: 0.5076\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\n",
            "MLP (Embeddings) Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9282    0.9425    0.9353      1878\n",
            "           1     0.7057    0.6540    0.6789       396\n",
            "\n",
            "    accuracy                         0.8923      2274\n",
            "   macro avg     0.8169    0.7983    0.8071      2274\n",
            "weighted avg     0.8894    0.8923    0.8906      2274\n",
            "\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - accuracy: 0.7936 - loss: 0.5066 - val_accuracy: 0.8769 - val_loss: 0.3146\n",
            "Epoch 2/5\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 0.9261 - loss: 0.2120 - val_accuracy: 0.8824 - val_loss: 0.2972\n",
            "Epoch 3/5\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 24ms/step - accuracy: 0.9788 - loss: 0.0827 - val_accuracy: 0.8764 - val_loss: 0.3931\n",
            "Epoch 4/5\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.9909 - loss: 0.0339 - val_accuracy: 0.8802 - val_loss: 0.4841\n",
            "Epoch 5/5\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.9969 - loss: 0.0151 - val_accuracy: 0.8731 - val_loss: 0.5493\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\n",
            "CNN (Embeddings) Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9215    0.9441    0.9327      1878\n",
            "           1     0.7000    0.6187    0.6568       396\n",
            "\n",
            "    accuracy                         0.8874      2274\n",
            "   macro avg     0.8108    0.7814    0.7948      2274\n",
            "weighted avg     0.8829    0.8874    0.8846      2274\n",
            "\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 128ms/step - accuracy: 0.8093 - loss: 0.4960 - val_accuracy: 0.8121 - val_loss: 0.4849\n",
            "Epoch 2/5\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 122ms/step - accuracy: 0.8095 - loss: 0.4901 - val_accuracy: 0.8121 - val_loss: 0.4837\n",
            "Epoch 3/5\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 122ms/step - accuracy: 0.8075 - loss: 0.4919 - val_accuracy: 0.8121 - val_loss: 0.4832\n",
            "Epoch 4/5\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 126ms/step - accuracy: 0.8112 - loss: 0.4865 - val_accuracy: 0.8121 - val_loss: 0.4887\n",
            "Epoch 5/5\n",
            "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 125ms/step - accuracy: 0.8091 - loss: 0.4878 - val_accuracy: 0.8121 - val_loss: 0.4859\n",
            "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step\n",
            "\n",
            "LSTM (Embeddings) Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8259    1.0000    0.9046      1878\n",
            "           1     0.0000    0.0000    0.0000       396\n",
            "\n",
            "    accuracy                         0.8259      2274\n",
            "   macro avg     0.4129    0.5000    0.4523      2274\n",
            "weighted avg     0.6820    0.8259    0.7471      2274\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, Flatten, Conv1D, GlobalMaxPooling1D, LSTM, Dropout\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"tweets.csv\")\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'@[A-Za-z0-9_]+','', text)\n",
        "    text = re.sub(r'#','', text)\n",
        "    text = re.sub(r'http\\S+|www.\\S+','', text)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = text.split()\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "df['clean'] = df['text'].apply(clean_text)\n",
        "\n",
        "X = df['clean']\n",
        "y = df['target']\n",
        "\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "X_count = count_vectorizer.fit_transform(X)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
        "\n",
        "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "log_reg = LogisticRegression(max_iter=200)\n",
        "log_reg.fit(X_train_tfidf, y_train)\n",
        "y_pred_lr = log_reg.predict(X_test_tfidf)\n",
        "\n",
        "svm = SVC()\n",
        "svm.fit(X_train_tfidf, y_train)\n",
        "y_pred_svm = svm.predict(X_test_tfidf)\n",
        "\n",
        "def evaluate_model(y_true, y_pred, name=\"Model\"):\n",
        "    print(f\"\\n{name} Report:\")\n",
        "    print(classification_report(y_true, y_pred, digits=4))\n",
        "\n",
        "evaluate_model(y_test, y_pred_lr, \"Logistic Regression (TF-IDF)\")\n",
        "evaluate_model(y_test, y_pred_svm, \"SVM (TF-IDF)\")\n",
        "\n",
        "# =========================\n",
        "# 6. WORD EMBEDDINGS (Tokenizer + Padding)\n",
        "# =========================\n",
        "max_words = 10000\n",
        "max_len = 50\n",
        "embedding_dim = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X)\n",
        "sequences = tokenizer.texts_to_sequences(X)\n",
        "X_pad = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "X_train_pad, X_test_pad, y_train_pad, y_test_pad = train_test_split(X_pad, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# =========================\n",
        "# 7. DEEP LEARNING MODELS\n",
        "# =========================\n",
        "\n",
        "# ---- (a) MLP on averaged embeddings ----\n",
        "mlp = Sequential([\n",
        "    Embedding(max_words, embedding_dim, input_length=max_len),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "mlp.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "mlp.fit(X_train_pad, y_train_pad, epochs=5, batch_size=32, validation_split=0.2, verbose=1)\n",
        "mlp_pred = (mlp.predict(X_test_pad) > 0.5).astype(int)\n",
        "evaluate_model(y_test_pad, mlp_pred, \"MLP (Embeddings)\")\n",
        "\n",
        "# ---- (b) 1D CNN ----\n",
        "cnn = Sequential([\n",
        "    Embedding(max_words, embedding_dim, input_length=max_len),\n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "cnn.fit(X_train_pad, y_train_pad, epochs=5, batch_size=32, validation_split=0.2, verbose=1)\n",
        "cnn_pred = (cnn.predict(X_test_pad) > 0.5).astype(int)\n",
        "evaluate_model(y_test_pad, cnn_pred, \"CNN (Embeddings)\")\n",
        "\n",
        "# ---- (c) LSTM ----\n",
        "lstm = Sequential([\n",
        "    Embedding(max_words, embedding_dim, input_length=max_len),\n",
        "    LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "lstm.fit(X_train_pad, y_train_pad, epochs=5, batch_size=32, validation_split=0.2, verbose=1)\n",
        "lstm_pred = (lstm.predict(X_test_pad) > 0.5).astype(int)\n",
        "evaluate_model(y_test_pad, lstm_pred, \"LSTM (Embeddings)\")"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTnanEaPIEpK/abKoqEQIK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koushik980/NLP/blob/main/NLP_F_24_10_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0Al_7Ia8oET",
        "outputId": "2dc31172-00eb-4d1f-9557-e1f081273004"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Task 1: LDA Topic Tuning ===\n",
            "\n",
            "--- LDA with 3 topics ---\n",
            "Topic #1: team, venture, startup, scale, platform\n",
            "Topic #2: economic, tightens, quarter, policy, predict\n",
            "Topic #3: war, new, services, released, smartphones\n",
            "\n",
            "--- LDA with 5 topics ---\n",
            "Topic #1: team, new, scored, signed, striker\n",
            "Topic #2: war, soldiers, reported, lines, impact\n",
            "Topic #3: new, tech, services, released, innovation\n",
            "Topic #4: war, venture, scale, startup, platform\n",
            "Topic #5: economic, tightens, quarter, policy, predict\n",
            "\n",
            "--- LDA with 8 topics ---\n",
            "Topic #1: war, new, economic, infrastructure, reported\n",
            "Topic #2: stock, central, lending, influence, market\n",
            "Topic #3: war, ceasefire, areas, aid, negotiate\n",
            "Topic #4: war, new, economic, infrastructure, reported\n",
            "Topic #5: new, economic, services, released, tech\n",
            "Topic #6: team, trick, scored, signed, striker\n",
            "Topic #7: war, new, economic, infrastructure, reported\n",
            "Topic #8: venture, startup, scale, learning, platform\n",
            "\n",
            "Separation score for k=3: 1.0000\n",
            "Separation score for k=5: 0.9778\n",
            "Separation score for k=8: 0.8542\n",
            "\n",
            "Recommended k by separation score: 3\n",
            "Explanation: separation score = 1 - mean(pairwise Jaccard of top-word sets). Higher => less overlap => clearer separation.\n",
            "\n",
            "=== Task 2: WordNet Hypernyms & Hyponyms ===\n",
            "\n",
            "Chosen synset: Synset('war.n.01') - the waging of armed conflict against an enemy\n",
            "\n",
            "Hypernyms (broader terms):\n",
            "action, military action\n",
            "\n",
            "Hyponyms (narrower terms):\n",
            "BW, IW, bioattack, biologic attack, biological attack, biological warfare, chemical operations, chemical warfare, civil war, hot war, information warfare, international jihad, jehad, jihad, limited war, psychological warfare, war of nerves, world war\n",
            "\n",
            "Discussion: Hyponyms like 'civil war', 'world war i/ii', 'guerrilla warfare', 'cold war' can be used to build fine-grained subtopics:\n",
            " - 'world war i/ii' -> historical global conflicts subtopic\n",
            " - 'civil war' -> internal conflicts subtopic\n",
            " - 'guerrilla warfare' -> tactics/asymmetric warfare subtopic\n",
            "\n",
            "=== Task 3: Jaccard Similarity (Unigram vs Bigram) ===\n",
            "\n",
            "Document A: A major military war broke out in the region, leading to peace talks and international sanctions.\n",
            "Document B: Diplomats met to negotiate a ceasefire and humanitarian aid to war-affected areas.\n",
            "Document C (contrast): Tech companies released new smartphones and cloud services, boosting innovation in AI and data.\n",
            "\n",
            "Unigram-based Jaccard similarities:\n",
            "Jaccard(A, B) = 0.1667 (|I|=4, |U|=24)\n",
            "Jaccard(A, C) = 0.0741 (|I|=2, |U|=27)\n",
            "\n",
            "Bigram-based Jaccard similarities:\n",
            "Jaccard(A, B) = 0.0000 (|I|=0, |U|=27)\n",
            "Jaccard(A, C) = 0.0000 (|I|=0, |U|=28)\n",
            "\n",
            "Interpretation:\n",
            "- Unigram Jaccard measures shared words; can be high if documents share vocabulary.\n",
            "- Bigram Jaccard measures shared contiguous 2-word phrases; it captures phrase-level similarity and ordering.\n",
            "- Bigram similarity often gives a tighter sense of relatedness when phrase structure matters; unigram is coarser.\n",
            "\n",
            "Script finished.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "import itertools\n",
        "import re\n",
        "import sys\n",
        "\n",
        "# ---------------------------\n",
        "#  Example corpus (replace with your data if desired)\n",
        "# ---------------------------\n",
        "documents = [\n",
        "    \"The government passed a new law to improve economic growth and increase employment.\",\n",
        "    \"A major military war broke out in the region, leading to peace talks and international sanctions.\",\n",
        "    \"The football team won their championship after a dramatic penalty shootout in the final match.\",\n",
        "    \"Tech companies released new smartphones and cloud services, boosting innovation in AI and data.\",\n",
        "    \"Economic analysts predict inflation will fall next quarter as monetary policy tightens.\",\n",
        "    \"Soldiers reported on the front lines about the war's impact on civilians and infrastructure.\",\n",
        "    \"The startup raised venture capital to scale its machine learning platform for health care.\",\n",
        "    \"Local team signed a new striker who scored a hat-trick and delighted the fans.\",\n",
        "    \"Diplomats met to negotiate a ceasefire and humanitarian aid to war-affected areas.\",\n",
        "    \"Central bank decisions on interest rates influence the stock market and lending.\"\n",
        "]\n",
        "\n",
        "# ---------------------------\n",
        "#  Utilities\n",
        "# ---------------------------\n",
        "def jaccard(set_a, set_b):\n",
        "    if not set_a and not set_b:\n",
        "        return 1.0\n",
        "    inter = len(set_a & set_b)\n",
        "    uni = len(set_a | set_b)\n",
        "    return inter / uni if uni > 0 else 0.0\n",
        "\n",
        "def simple_tokenize(text):\n",
        "    \"\"\"Lowercase tokenization: returns list of word tokens (alphanumeric).\"\"\"\n",
        "    return re.findall(r\"\\b\\w+\\b\", text.lower())\n",
        "\n",
        "def make_ngrams(tokens, n=2):\n",
        "    return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
        "\n",
        "# ---------------------------\n",
        "#  Task 1: LDA with 3,5,8 topics\n",
        "# ---------------------------\n",
        "def run_lda_and_print(documents, topic_nums=(3,5,8), top_n_words=5):\n",
        "    print(\"=== Task 1: LDA Topic Tuning ===\\n\")\n",
        "    vectorizer = CountVectorizer(stop_words='english', max_df=0.9, min_df=1)\n",
        "    X = vectorizer.fit_transform(documents)\n",
        "    feature_names = np.array(vectorizer.get_feature_names_out())\n",
        "\n",
        "    lda_topics = {}\n",
        "    for k in topic_nums:\n",
        "        lda = LDA(n_components=k, random_state=42, max_iter=100)\n",
        "        lda.fit(X)\n",
        "        print(f\"--- LDA with {k} topics ---\")\n",
        "        topics = []\n",
        "        for topic_idx, topic in enumerate(lda.components_):\n",
        "            top_indices = topic.argsort()[::-1][:top_n_words]\n",
        "            top_words = feature_names[top_indices]\n",
        "            topics.append(list(top_words))\n",
        "            print(f\"Topic #{topic_idx+1}: {', '.join(top_words)}\")\n",
        "        lda_topics[k] = topics\n",
        "        print(\"\")\n",
        "    # compute separation metric: 1 - mean(pairwise Jaccard on top-10 words)\n",
        "    def topic_separation_score(topics, top_n=10):\n",
        "        sets = [set(t[:top_n]) for t in topics]\n",
        "        pairs = list(itertools.combinations(range(len(sets)), 2))\n",
        "        if not pairs:\n",
        "            return 0.0\n",
        "        sims = [jaccard(sets[i], sets[j]) for i,j in pairs]\n",
        "        return 1 - np.mean(sims)\n",
        "    scores = {k: topic_separation_score(tpcs, top_n=10) for k,tpcs in lda_topics.items()}\n",
        "    best_k = max(scores, key=scores.get)\n",
        "    for k, sc in scores.items():\n",
        "        print(f\"Separation score for k={k}: {sc:.4f}\")\n",
        "    print(f\"\\nRecommended k by separation score: {best_k}\")\n",
        "    print(\"Explanation: separation score = 1 - mean(pairwise Jaccard of top-word sets). Higher => less overlap => clearer separation.\\n\")\n",
        "    return lda_topics, scores\n",
        "\n",
        "# ---------------------------\n",
        "#  Task 2: WordNet Hypernyms & Hyponyms for a keyword\n",
        "# ---------------------------\n",
        "def wordnet_hyper_hypo(keyword='war'):\n",
        "    print(\"=== Task 2: WordNet Hypernyms & Hyponyms ===\\n\")\n",
        "    hypernyms = set()\n",
        "    hyponyms = set()\n",
        "    try:\n",
        "        import nltk\n",
        "        from nltk.corpus import wordnet as wn\n",
        "        # ensure wordnet is downloaded\n",
        "        try:\n",
        "            wn.ensure_loaded()\n",
        "        except Exception:\n",
        "            nltk.download('wordnet', quiet=True)\n",
        "        synsets = wn.synsets(keyword)\n",
        "        if not synsets:\n",
        "            print(f\"No WordNet synsets found for '{keyword}'.\\n\")\n",
        "            raise RuntimeError(\"no synsets\")\n",
        "        # choose first (often most common) synset for demonstration\n",
        "        s = synsets[0]\n",
        "        print(\"Chosen synset:\", s, \"-\", s.definition())\n",
        "        for h in s.hypernyms():\n",
        "            for lemma in h.lemmas():\n",
        "                hypernyms.add(lemma.name().replace('_', ' '))\n",
        "        for hypo in s.hyponyms():\n",
        "            for lemma in hypo.lemmas():\n",
        "                hyponyms.add(lemma.name().replace('_', ' '))\n",
        "        print(\"\\nHypernyms (broader terms):\")\n",
        "        print(\", \".join(sorted(hypernyms)) if hypernyms else \"None found\")\n",
        "        print(\"\\nHyponyms (narrower terms):\")\n",
        "        print(\", \".join(sorted(hyponyms)) if hyponyms else \"None found\")\n",
        "    except Exception as e:\n",
        "        # fallback mapping if wordnet isn't available\n",
        "        print(\"WordNet not available or error occurred:\", str(e))\n",
        "        print(\"Using fallback illustrative lists.\\n\")\n",
        "        hypernyms = {\"conflict\", \"hostility\"}\n",
        "        hyponyms = {\"civil war\", \"world war i\", \"world war ii\", \"guerrilla warfare\", \"cold war\", \"civil unrest\"}\n",
        "        print(\"Hypernyms (fallback):\", \", \".join(sorted(hypernyms)))\n",
        "        print(\"Hyponyms (fallback):\", \", \".join(sorted(hyponyms)))\n",
        "    # short discussion\n",
        "    print(\"\\nDiscussion: Hyponyms like 'civil war', 'world war i/ii', 'guerrilla warfare', 'cold war' can be used to build fine-grained subtopics:\")\n",
        "    print(\" - 'world war i/ii' -> historical global conflicts subtopic\")\n",
        "    print(\" - 'civil war' -> internal conflicts subtopic\")\n",
        "    print(\" - 'guerrilla warfare' -> tactics/asymmetric warfare subtopic\")\n",
        "    print()\n",
        "    return hypernyms, hyponyms\n",
        "\n",
        "# ---------------------------\n",
        "#  Task 3: Jaccard similarity with bigrams vs unigrams\n",
        "# ---------------------------\n",
        "def jaccard_unigram_bigram_compare(doc_a, doc_b, doc_c=None):\n",
        "    print(\"=== Task 3: Jaccard Similarity (Unigram vs Bigram) ===\\n\")\n",
        "    print(\"Document A:\", doc_a)\n",
        "    print(\"Document B:\", doc_b)\n",
        "    if doc_c:\n",
        "        print(\"Document C (contrast):\", doc_c)\n",
        "    print(\"\")\n",
        "\n",
        "    tokens_a = simple_tokenize(doc_a)\n",
        "    tokens_b = simple_tokenize(doc_b)\n",
        "    tokens_c = simple_tokenize(doc_c) if doc_c else None\n",
        "\n",
        "    unigrams_a = set(tokens_a)\n",
        "    unigrams_b = set(tokens_b)\n",
        "    unigrams_c = set(tokens_c) if tokens_c else None\n",
        "\n",
        "    bigrams_a = set(make_ngrams(tokens_a, 2))\n",
        "    bigrams_b = set(make_ngrams(tokens_b, 2))\n",
        "    bigrams_c = set(make_ngrams(tokens_c, 2)) if tokens_c else None\n",
        "\n",
        "    def print_jacc(name1, name2, s1, s2):\n",
        "        print(f\"Jaccard({name1}, {name2}) = {jaccard(s1,s2):.4f} (|I|={len(s1 & s2)}, |U|={len(s1 | s2)})\")\n",
        "\n",
        "    print(\"Unigram-based Jaccard similarities:\")\n",
        "    print_jacc(\"A\",\"B\", unigrams_a, unigrams_b)\n",
        "    if unigrams_c is not None:\n",
        "        print_jacc(\"A\",\"C\", unigrams_a, unigrams_c)\n",
        "\n",
        "    print(\"\\nBigram-based Jaccard similarities:\")\n",
        "    print_jacc(\"A\",\"B\", bigrams_a, bigrams_b)\n",
        "    if bigrams_c is not None:\n",
        "        print_jacc(\"A\",\"C\", bigrams_a, bigrams_c)\n",
        "\n",
        "    print(\"\\nInterpretation:\")\n",
        "    print(\"- Unigram Jaccard measures shared words; can be high if documents share vocabulary.\")\n",
        "    print(\"- Bigram Jaccard measures shared contiguous 2-word phrases; it captures phrase-level similarity and ordering.\")\n",
        "    print(\"- Bigram similarity often gives a tighter sense of relatedness when phrase structure matters; unigram is coarser.\")\n",
        "    print()\n",
        "\n",
        "# ---------------------------\n",
        "#  Main execution\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Task 1\n",
        "    lda_topics, scores = run_lda_and_print(documents, topic_nums=(3,5,8), top_n_words=5)\n",
        "\n",
        "    # Task 2\n",
        "    hypernyms, hyponyms = wordnet_hyper_hypo('war')\n",
        "\n",
        "    # Task 3 - pick two documents (war-related) and a contrast (tech)\n",
        "    docA = documents[1]  # war\n",
        "    docB = documents[8]  # war/peace negotiations\n",
        "    docC = documents[3]  # tech doc (contrast)\n",
        "    jaccard_unigram_bigram_compare(docA, docB, docC)\n",
        "\n",
        "    print(\"Script finished.\")"
      ]
    }
  ]
}